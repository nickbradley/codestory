\documentclass[../manifest.tex]{subfiles}

\begin{document}

Code reviews are an essential part of modern software engineering and we know that the quality of code reviews is a key factor to overall software quality. In order to evaluate whether CodeStory supports developers in program understanding tasks, we thus approached participants with code reviews with and without our tool (section \ref{eval-description}) and analyzed whether the quality of the code reviews improved with our tool (section \ref{eval-impact}). Before this study, however, we determined the general feasibility of CodeStory by performing a pilot survey among industry developers who perform code reviews on a daily basis (section \ref{eval-survey}). Threats to validity for our evaluation are described in section \ref{eval-threats}.

\subsection{Pilot Survey} \label{eval-survey}

At an early stage of this paper, we aimed to determine whether our assumption that it is useful to record developers' reasoning during coding tasks is true. Additionally, we wanted to collect information on what information may in fact be useful. We therefore collected 17 responses of industry developers who regularly perform code reviews with a survey. 

After providing a brief introduction of our research, we described the following scenario: \textit{a developer has to choose a sort algorithm for a particular task. She googles "sort array in JavaScript" and finds a code snippet on StackOverflow. She copies the snippet as a scaffold into her code}.

We then let participants rate the following question on a scale of 1 (not useful) to 5 (very useful): \textit{How useful would the story above be during the code review?} The responses to this question were positive: no participant rated with 0 and only 2 participants rated with 1 (negative responses). 6 participants rated with 3 (neutral), 7 participants rated with 4 and 2 participants rated with 5 (positive). So overall, 88\% responded neutral or positive to the question whether recording the described context information would be useful.

Finally, we collected opinions on the particular information of interest using the same scale from 1 to 5: \textit{What elements from the above story would you consider useful?} Table~\ref{tab:elements-of-interest} shows the list of elements and the percentage of the responses that rated with either 3 (maybe useful), 4 (somewhat useful) or 5 (very useful).

Considering the majority of neutral and positive responses to our pilot survey we felt confirmed that the idea behind CodeStory is valuable and we could continue our research. The single element that had a overly negative result in the survey was the \textit{Google search query leading to StackOverflow}. We consequently neglected this element for the implementation of CodeStory.

\begin{table*}[t]
    \label{tab:elements-of-interest}
    \centering
    \begin{threeparttable}
    \begin{tabular*}{\textwidth}{ll}
    \hline
    \textbf{Element} & \textbf{Percentage of 3-5 ratings} \\
    \hline
    Google search query leading to StackOverflow & 24\% \\
    StackOverflow question URL & 76\% \\
    StackOverflow question heading & 47\% \\
    StackOverflow question content & 76\% \\
    StackOverflow answer URL & 76\% \\
    Time of access of StackOverflow page & 59\% \\
    StackOverflow answer code snippet & 53\% \\
    Entire StackOverflow answer & 47\% \\
    StackOverflow answer rating & 59\% \\
    StackOverflow answer acceptance status & 59\% \\
    StackOverflow answer comments & 59\% \\
    Other StackOverflow answers & 47\% \\
    \hline
    \end{tabular*}
    \end{threeparttable}
    \caption{Elements of interest for survey rating}
\end{table*}

\subsection{Study Description} \label{eval-description}

In order to determine the usefulness of CodeStory in practice, we approached developers with code reviews. We created a bootstrap repository on Github that contained a simple Maven Java-Project. We aimed to simulate a realistic scenario with two changes that are induced by a developer's research on StackOverflow and a subsequent copy/paste operation. We chose two questions on StackOverflow as scenarios for the study:
\begin{enumerate}
  \item \textit{Gson - convert from Json to a typed ArrayList<T>}\footnote{http://stackoverflow.com/questions/12384064/gson-convert-from-json-to-a-typed-arraylistt}
  \item \textit{How do you compare two version Strings in Java?}\footnote{http://stackoverflow.com/questions/198431/how-do-you-compare-two-version-strings-in-java}
\end{enumerate}

In (1), a developer was struggling with deserializing a JSON string to an instance of a custom Java class using the google-gson\footnote{https://github.com/google/gson} library. The developer tries to pass a class object for the generic type \code{ArrayList<JsonLog>} by passing \code{ArrayList<JsonLog>.class} to \code{fromJson} (Gson's method for deserializing a JSON string), which is no valid Java code and does not compile (because class objects cannot be created from generics that way in Java). The first answer to the question, which is also the accepted answer for the question, describes how to solve this problem by using Gson's \code{TypeToken} class for creating the desired class object: \code{new TypeToken<List<JsonLog> >(){}.getType()}. Figure~\ref{fig:pull-request-1} shows the diff of the change that we created for this scenario.
% TODO: make >> work in code!!

\begin{figure}[h]
  \centering
  \includegraphics[width=\linewidth]{pull-request-1}
  \caption{Diff view for study scenario 1}
  \label{fig:pull-request-1}
\end{figure}

In (2), a developer was interested in how to compare version strings semantically. This question is associated with \textit{semantic versioning}\footnote{http://semver.org/} which defines what versions should be considered higher than others, i.e. \textit{1.0.0-alpha < 1.0.0-alpha.1 < 1.0.0-beta < 1.0.0-rc.1 < 1.0.0}. More specifically, the question was: \textit{given two version strings a and b, which one is higher?} For this scenario, we chose a lower ranked solution\footnote{http://stackoverflow.com/a/27891752/1105907} as a base version to be replaced by a higher ranked solution\footnote{http://stackoverflow.com/a/6640972/1105907} as the improved version. Both solutions are answers on the same given question on StackOverflow but in this case neither of the two is the accepted answer because the accepted answer in that case does not contain any code. Figures~\ref{fig:pull-request-2-1} and~\ref{fig:pull-request-2-2} show the diff of the change that we created for this scenario.

% TODO: say something why simple question-answer in scenario 1 but somewhat hidden answer content in scenario 2?

\begin{figure}[h]
  \label{fig:pull-request-2-1}
  \centering
  \includegraphics[width=\linewidth]{pull-request-2-1}
  \caption{Diff view for study scenario 2 (part 1)}
\end{figure}

\begin{figure}[h]
  \label{fig:pull-request-2-2}
  \centering
  \includegraphics[width=\linewidth]{pull-request-2-2}
  \caption{Diff view for study scenario 2 (part 2)}
\end{figure}

For both scenarios we now added CodeStory to the developer workflow. The resulting CodeStory pages are shown in Figures~\ref{fig:codestory-page-1} and~\ref{fig:codestory-page-2}.

\begin{figure}[h]
  \label{fig:codestory-page-1}
  \centering
  \includegraphics[width=\linewidth]{codestory-page-1}
  \caption{CodeStory page for scenario 1}
\end{figure}

\begin{figure}[h]
  \label{fig:codestory-page-2}
  \centering
  \includegraphics[width=\linewidth]{codestory-page-2}
  \caption{CodeStory page for scenario 2}
\end{figure}

For the changes described with scenario 1 and 2 we now created another version for each that includes a link to the respective CodeStory page as URL in a comment above the pasted content. Since scenario 2 contained two paste operations (one in an XML file and one in a Java file), two such comments were added respectively. This constellation gave us four treatments for our study:
\begin{itemize}
  \item \textbf{T1:} Scenario 1 without annotation
  \item \textbf{T2:} Scenario 2 without annotation
  \item \textbf{T3:} Scenario 1 with annotation
  \item \textbf{T4:} Scenario 2 with annotation
\end{itemize}

The study was performed with 8 developers among which 4 participants were graduate students and 4 were industry developers (> 4 years of practical experience with Java). With participants from different backgrounds we ensured generalizability of our study for both academia and industry. Due to time constraints we decided to assign each participant 2 code reviews (we assumed ~10min for each code review). To avoid learning bias for the given scenarios and treatments, we decided to let each participant do one code review for each scenario, one with annotation and one without annotation. To further avoid learning bias, we ensured that 4 participants start with a code review with annotation and the other 4 start with a code review without annotation. Table~\ref{tab:study-outline} shows the outline of treatments and participants.

\begin{table*}[t]
    \label{tab:study-outline}
    \centering
    \begin{threeparttable}
    \begin{tabular*}{\textwidth}{lll}
    \hline
    \textbf{Participant} & \textbf{Treatment} & \textbf{Background} \\
    \hline
    P1 & T3 + T2 & industry \\
    P2 & T3 + T2 & academia \\
    P3 & T1 + T4 & industry \\
    P4 & T1 + T4 & academia \\
    P5 & T4 + T1 & industry \\
    P6 & T4 + T1 & academia \\
    P7 & T2 + T3 & industry \\
    P8 & T2 + T3 & academia \\
    \hline
    \end{tabular*}
    \end{threeparttable}
    \caption{Study Outline}
\end{table*}

For each participant we forked our bootstrap repository and created two pull requests with respect to their treatments. In the description of each pull requests we asked the following questions:
\begin{enumerate}
  \item \textit{What is the purpose of the method with the change?}
  \item \textit{How did the method change?}
  \item \textit{Why was this change made?}
\end{enumerate}

We then added the participants as reviewers and instructed them to perform their code reviews in sequential order by answering our questions as comments on the pull requests. The code reviews were thus performed online and no time constraints besides the sequential order were given since we did not see any benefits in imposing further restrictions on participants.

After the second pull request, participants were finally approached with a survey to determine to what degree CodeStory helped them during the code review (for the annotated pull request) and whether they found the tool generally useful. We first asked participants to rank their experience for each pull request on a scale from 1 (very challenging) to 5 (very easy). We then asked \textit{Was the information provided by CodeStory useful?} Table~\ref{tab:survey-results} shows the results of the survey with the ratings of each participant and the associated treatment. Finally, we gave participants the option to comment on our tool: \textit{Do you have any comments about CodeStory? Were you missing any information?}

With the analysis of the survey results, we found two \textbf{problems} of our study: (1) one participant (P4) did the pull requests in opposite order. We consider this a minor issue that does not affect the results. But (2) we consider major: one participant (P8) did not notice the CodeStory link and the associated result is therefore of no value. We therefore neglect P8 in the subsequent parts of this paper.

\begin{table*}[t]
    \label{tab:survey-results}
    \centering
    \begin{threeparttable}
    \begin{tabular*}{\textwidth}{llllll}
    \hline
    \textbf{} & \textbf{T1} & \textbf{T2} & \textbf{T3} & \textbf{T4} & \textbf{CodeStory Rating} \\
    \hline
    P1 &   & 2 & 4 &   & 5 \\
    P2 &   & 2 & 4 &   & 5 \\
    P3 & 4 &   &   & 5 & 5 \\
    P4 & 1 &   &   & 5 & 4 \\
    P5 & 2 &   &   & 5 & 5 \\
    P6 & 3 &   &   & 5 & 5 \\
    P7 &   & 5 & 3 &   & 4 \\
    P8* &   & 4 & 2 &   & 2 \\
    \hline
    \end{tabular*}
    \begin{tablenotes}\footnotesize
      \item [*] Neglected in subsequent sections because participant did not notice the CodeStory tool.
    \end{tablenotes}
    \end{threeparttable}
    \caption{Survey results}
\end{table*}

\subsection{Impact for Code Review Tasks} \label{eval-impact}

Comparing code review comments without (T1/T2) and with CodeStory annotation (T3/T4), we could clearly see an increase in quality using our tool. Basically, T1 and T2 induce great amounts of guessing in finding the right answers to our questions, primarily visible through phrases like "presumably", "it appears to be", "it seems to me", "I think", "I believe", etc. A decrease in quality in answers to our questions (1) \textit{What is the purpose of the method with the change?}, (2) \textit{How did the method change?}, (3) \textit{Why was this change made?} is visible for T1/T2: while (1) could often be answered relatively easily, (2) proved very hard and (3) proved nearly impossible. That is why we observe most guessing with (3). Some participants tried to do their own online research for the questions on T1/T2 but were mostly still unable to provide good answers. Others commented that question 3 is not answerable for T1/T2. While we could generally observe higher quality answers from industry developers than from graduate students, the relationship between code reviews with and without our tool remains the same and we can therefore neglect variance resulting from participants' backgrounds.

In contrast, comments on code reviews with CodeStory (T3/T4) showed significantly higher quality. Most participants provided very good answers to our questions for these treatments. The additional contextual information provided by CodeStory clearly helped them to answer what "really happened" for the given code changed, i.e. what the developer's reasoning was during implementation. The following list provides representative examples of code review comments with answers to our questions for each treatment:

\begin{itemize}
  \item \textbf{T1:}
\textit{(1) I think that the method is trying to convert a json object into a gson object.
(2) It changed through the usage of the TypedToken List and anonymous class.
(3) The method is really small, though I believe that it was changed to consider the usage of interfaces and generics. As far as I remember, there is an issue trying to get the type of  ArrayList<JsonLog>.class and the correct implementation would have to create an anonymous class.}
  \item \textbf{T2:} XXX
  \item \textbf{T3:} XXX
  \item \textbf{T4:} XXX
\end{itemize}

% much guessing in - (presumably, it appears to be, it seems to me, I think, I believe...)
% quality of comments decreases from 1 to 3 in -
% quality of industry developers comments is generally higher but trend of - to + are the same
% some googled
% some commented that answering is not possible without further information on -

% examples for treatments

\subsection{Threads to Validity} \label{eval-threats}

% problems 1 and 2 described
% what is higher quality? does this really lead to higher software quality?

\end{document}
