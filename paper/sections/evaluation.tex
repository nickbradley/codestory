\documentclass[../manifest.tex]{subfiles}

\begin{document}

Code reviews are an essential part of modern software engineering and we know that the quality of code reviews is a key factor to overall software quality. In order to evaluate whether CodeStory supports developers in program understanding tasks, we thus approached participants with code reviews with and without our tool (section \ref{eval-description}) and analyzed whether the quality of the code reviews improved with our tool (section \ref{eval-impact}). Before this study, however, we determined the general feasibility of CodeStory by performing a pilot survey among industry developers who perform code reviews on a daily basis (section \ref{eval-survey}). Threats to validity for our evaluation are described in section \ref{eval-threats}.

\subsection{Pilot Survey} \label{eval-survey}

At an early stage of this paper, we aimed to determine whether our assumption that it is useful to record developers' reasoning during coding tasks is true. Additionally, we wanted to collect information on what information may in fact be useful. We therefore approached 17 industry developers that perform code reviews on a daily basis with a survey. 

After providing a brief introduction of our research, we described the following scenario: \textit{a developer has to choose a sort algorithm for a particular task. She googles 'sort array in JavaScript' and finds a code snippet on StackOverflow. She copies the snippet as a scaffold into her code}.

We then let participants rate the following question on a scale of 1 (not useful) to 5 (very useful): \textit{How useful would the story above be during the code review?}.

Finally, we collected opinions on what elements may be useful using the same scale from 1 to 5. The list of elements is shown in table ~\ref{XXX}.

\subsection{Study Description} \label{eval-description}

\subsection{Impact for Code Review Tasks} \label{eval-impact}

\subsection{Threads to Validity} \label{eval-threats}

\end{document}
